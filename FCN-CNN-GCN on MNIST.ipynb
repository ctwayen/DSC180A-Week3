{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is edited based on https://github.com/bknyaz/examples/blob/master/fc_vs_graph_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import section\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models; @author: https://github.com/bknyaz/examples/blob/master/fc_vs_graph_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BorisNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BorisNet, self).__init__()\n",
    "        self.fc = nn.Linear(784, 10, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x.view(x.size(0), -1))\n",
    "\n",
    "\n",
    "class BorisConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BorisConvNet, self).__init__()\n",
    "        self.conv = nn.Conv2d(1, 10, 28, stride=1, padding=14)\n",
    "        self.fc = nn.Linear(4 * 4 * 10, 10, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = F.max_pool2d(x, 7)\n",
    "        return self.fc(x.view(x.size(0), -1))\n",
    "\n",
    "class BorisGraphNet(nn.Module):\n",
    "    def __init__(self, img_size=28, pred_edge=False):\n",
    "        super(BorisGraphNet, self).__init__()\n",
    "        self.pred_edge = pred_edge\n",
    "        N = img_size ** 2\n",
    "        self.fc = nn.Linear(N, 10, bias=False)\n",
    "        if pred_edge:\n",
    "            col, row = np.meshgrid(np.arange(img_size), np.arange(img_size))\n",
    "            coord = np.stack((col, row), axis=2).reshape(-1, 2)\n",
    "            coord = (coord - np.mean(coord, axis=0)) / (np.std(coord, axis=0) + 1e-5)\n",
    "            coord = torch.from_numpy(coord).float()  # 784,2\n",
    "            coord = torch.cat((coord.unsqueeze(0).repeat(N, 1,  1),\n",
    "                                    coord.unsqueeze(1).repeat(1, N, 1)), dim=2)\n",
    "            #coord = torch.abs(coord[:, :, [0, 1]] - coord[:, :, [2, 3]])\n",
    "            self.pred_edge_fc = nn.Sequential(nn.Linear(4, 64),\n",
    "                                              nn.ReLU(),\n",
    "                                              nn.Linear(64, 1),\n",
    "                                              nn.Tanh())\n",
    "            self.register_buffer('coord', coord)\n",
    "        else:\n",
    "            # precompute adjacency matrix before training\n",
    "            A = self.precompute_adjacency_images(img_size)\n",
    "            self.register_buffer('A', A)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def precompute_adjacency_images(img_size):\n",
    "        col, row = np.meshgrid(np.arange(img_size), np.arange(img_size))\n",
    "        coord = np.stack((col, row), axis=2).reshape(-1, 2) / img_size\n",
    "        dist = cdist(coord, coord)  \n",
    "        sigma = 0.05 * np.pi\n",
    "        \n",
    "        # Below, I forgot to square dist to make it a Gaussian (not sure how important it can be for final results)\n",
    "        A = np.exp(- dist / sigma ** 2)\n",
    "        print('WARNING: try squaring the dist to make it a Gaussian')\n",
    "            \n",
    "        A[A < 0.01] = 0\n",
    "        A = torch.from_numpy(A).float()\n",
    "\n",
    "        # Normalization as per (Kipf & Welling, ICLR 2017)\n",
    "        D = A.sum(1)  # nodes degree (N,)\n",
    "        D_hat = (D + 1e-5) ** (-0.5)\n",
    "        A_hat = D_hat.view(-1, 1) * A * D_hat.view(1, -1)  # N,N\n",
    "\n",
    "        # Some additional trick I found to be useful\n",
    "        A_hat[A_hat > 0.0001] = A_hat[A_hat > 0.0001] - 0.2\n",
    "\n",
    "        print(A_hat[:10, :10])\n",
    "        return A_hat\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        if self.pred_edge:\n",
    "            self.A = self.pred_edge_fc(self.coord).squeeze()\n",
    "\n",
    "        avg_neighbor_features = (torch.bmm(self.A.unsqueeze(0).expand(B, -1, -1),\n",
    "                                 x.view(B, -1, 1)).view(B, -1))\n",
    "        return self.fc(avg_neighbor_features)\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training FCN, CNN, GCN on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting global parameters\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# set the train and test loader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=64, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "        batch_size=1000, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BorisNet(\n",
      "  (fc): Linear(in_features=784, out_features=10, bias=False)\n",
      ")\n",
      "number of trainable parameters: 7840\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.436514\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.107640\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.764128\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.575390\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.850568\n",
      "\n",
      "Test set: Average loss: 0.5874, Accuracy: 8653/10000 (87%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.678460\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.646225\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.656393\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.535244\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.632265\n",
      "\n",
      "Test set: Average loss: 0.4666, Accuracy: 8847/10000 (88%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.367931\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.553196\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.380436\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.466089\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.504286\n",
      "\n",
      "Test set: Average loss: 0.4191, Accuracy: 8905/10000 (89%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.421613\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.469619\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.540862\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.509709\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.341318\n",
      "\n",
      "Test set: Average loss: 0.3914, Accuracy: 8948/10000 (89%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.495211\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.443077\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.301002\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.503412\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.267667\n",
      "\n",
      "Test set: Average loss: 0.3732, Accuracy: 8980/10000 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run FCN\n",
    "# args: learning rate: 1e-3, weight_decay: 1e-4, epoch:5\n",
    "model_FCN = BorisNet()\n",
    "model_FCN.to(device)\n",
    "print(model_FCN)\n",
    "optimizer = optim.SGD(model_FCN.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "print('number of trainable parameters: %d' %\n",
    "    np.sum([np.prod(p.size()) if p.requires_grad else 0 for p in model_FCN.parameters()]))\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train(model_FCN, device, train_loader, optimizer, epoch)\n",
    "    test(model_FCN, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BorisConvNet(\n",
      "  (conv): Conv2d(1, 10, kernel_size=(28, 28), stride=(1, 1), padding=(14, 14))\n",
      "  (fc): Linear(in_features=160, out_features=10, bias=False)\n",
      ")\n",
      "number of trainable parameters: 9450\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.336543\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.001802\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.711546\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.568267\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.247959\n",
      "\n",
      "Test set: Average loss: 1.0589, Accuracy: 8547/10000 (85%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.946274\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.908382\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.674974\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.766695\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.789039\n",
      "\n",
      "Test set: Average loss: 0.5599, Accuracy: 9036/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.512419\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.711134\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.439335\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.525190\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.448992\n",
      "\n",
      "Test set: Average loss: 0.4085, Accuracy: 9238/10000 (92%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.486695\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.338398\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.413376\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.376810\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.490588\n",
      "\n",
      "Test set: Average loss: 0.3432, Accuracy: 9344/10000 (93%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.317167\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.400440\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.269931\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.431394\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.406783\n",
      "\n",
      "Test set: Average loss: 0.3083, Accuracy: 9407/10000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#run CNN\n",
    "#args: learning rate: 1e-3, weight_decay: 1e-1, epoch:5\n",
    "model_CNN = BorisConvNet()\n",
    "model_CNN.to(device)\n",
    "print(model_CNN)\n",
    "optimizer = optim.SGD(model_CNN.parameters(), lr=1e-3, weight_decay=1e-1)\n",
    "print('number of trainable parameters: %d' %\n",
    "    np.sum([np.prod(p.size()) if p.requires_grad else 0 for p in model_CNN.parameters()]))\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train(model_CNN, device, train_loader, optimizer, epoch)\n",
    "    test(model_CNN, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: try squaring the dist to make it a Gaussian\n",
      "tensor([[ 0.3400, -0.0852, -0.1736, -0.1938,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [-0.0852,  0.2413, -0.0987, -0.1763, -0.1944,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [-0.1736, -0.0987,  0.2207, -0.1015, -0.1768, -0.1946,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [-0.1938, -0.1763, -0.1015,  0.2166, -0.1020, -0.1770, -0.1946,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000, -0.1944, -0.1768, -0.1020,  0.2166, -0.1020, -0.1770, -0.1946,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000, -0.1946, -0.1770, -0.1020,  0.2166, -0.1020, -0.1770,\n",
      "         -0.1946,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000, -0.1946, -0.1770, -0.1020,  0.2166, -0.1020,\n",
      "         -0.1770, -0.1946],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1946, -0.1770, -0.1020,  0.2166,\n",
      "         -0.1020, -0.1770],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1946, -0.1770, -0.1020,\n",
      "          0.2166, -0.1020],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1946, -0.1770,\n",
      "         -0.1020,  0.2166]])\n",
      "BorisGraphNet(\n",
      "  (fc): Linear(in_features=784, out_features=10, bias=False)\n",
      ")\n",
      "number of trainable parameters: 7840\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 4.356149\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.475141\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.516558\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.479055\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.346513\n",
      "\n",
      "Test set: Average loss: 0.3872, Accuracy: 8854/10000 (89%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.475218\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.273606\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.319141\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.299997\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.299493\n",
      "\n",
      "Test set: Average loss: 0.3541, Accuracy: 8973/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.401842\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.281201\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.310288\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.357705\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.252229\n",
      "\n",
      "Test set: Average loss: 0.3414, Accuracy: 8996/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.456032\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.587390\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.312030\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.347597\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.484789\n",
      "\n",
      "Test set: Average loss: 0.3290, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.204154\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.178136\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.421097\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.418061\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.269636\n",
      "\n",
      "Test set: Average loss: 0.3240, Accuracy: 9069/10000 (91%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run GCN\n",
    "# args: learning rate: 1e-3, weight_decay: 1e-4, epoch:5\n",
    "model_GCN = BorisGraphNet(pred_edge=False)\n",
    "model_GCN.to(device)\n",
    "print(model_GCN)\n",
    "optimizer = optim.SGD(model_GCN.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "print('number of trainable parameters: %d' %\n",
    "    np.sum([np.prod(p.size()) if p.requires_grad else 0 for p in model_GCN.parameters()]))\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train(model_GCN, device, train_loader, optimizer, epoch)\n",
    "    test(model_GCN, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
